{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b8fe800",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    " it turns out that gradient descent is an algorithm that you can use to try to minimize any function, not just a cost function for linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44daf5a6",
   "metadata": {},
   "source": [
    "## Outline\n",
    "- Start with some $w$ and $b$ (set them to $0$)\n",
    "- Keep changing $w,b$ to reduce $J(w,b)$ until we settle at or near a minimum (may have more than one minimum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea45861",
   "metadata": {},
   "source": [
    "## Implementing Gradient Descent\n",
    "\n",
    "Simultaneously update $w$ and $b$. Repeat until convergence (we reach the point at a local minimum where the parameters $w$ and $b$ no longer change much with each additional step that we take).\n",
    "\n",
    "$$ w = w - \\alpha \\frac{\\partial}{\\partial w}J(w, b)$$\n",
    "$$ b = b - \\alpha \\frac{\\partial}{\\partial b}J(w, b)$$\n",
    "\n",
    "- $\\alpha$ is the Learning rate. It controls how big of a step you take when updating the model's parameters, $w$ and $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003f1ef3",
   "metadata": {},
   "source": [
    "## Correct implementation in coding: Simultaneous update\n",
    "\n",
    "$$ tmp_w=w - \\alpha \\frac{\\partial}{\\partial w}J(w, b)$$\n",
    "$$ tmp_b=b - \\alpha \\frac{\\partial}{\\partial b}J(w, b)$$\n",
    "$$w=tmp_w$$\n",
    "$$b=tmp_b$$\n",
    "\n",
    "## Incorrect:\n",
    "$$ tmp_w=w - \\alpha \\frac{\\partial}{\\partial w}J(w, b)$$\n",
    "$$w=tmp_w$$\n",
    "$$ tmp_b=b - \\alpha \\frac{\\partial}{\\partial b}J(w, b)$$\n",
    "$$b=tmp_b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee94d19",
   "metadata": {},
   "source": [
    ">**Note**: as we get nearer a local minimum gradient descent will automatically take smaller steps. And that's because as we approach the local minimum, the derivative automatically gets smaller. And that means the update steps also automatically gets smaller. \n",
    "\n",
    "## Near a local minimum\n",
    "- Derivative becomes smaller\n",
    "- Update steps becomes smaller\n",
    "\\\n",
    "Can reach minimum withouth decreasing the learning rate $\\alpha$ (in case $\\alpha$ is kept at some fixed value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dfa989",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
